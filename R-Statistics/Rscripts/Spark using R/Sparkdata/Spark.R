
## install.packages("sparklyr")
## packageVersion("sparklyr")
library('sparklyr')

## returns a path with spaces
getwd()  
  ## reset path
  ## setwd("path")

## Spark??????Scala????????????(???Java?????????(JVM)??????)?????????,????????????????????????????????????Java 8
system("java -version") 

## ???Spark 2.3?????????,?????????????????????????????????,??????????????????????????????????????????,???????????????????????????
spark_install("2.3")
  ## display all of the versions of Spark that are available for installation
  ## spark_available_versions()

## ???????????????????????????
spark_installed_versions()

  ## ????????????????????????Spark,??????????????????Spark???Hadoop???????????????spark_uninstall()
  ## spark_uninstall(version = "1.6.3", hadoop = "2.6")

## ????????????????????????
## ??????????????????????????????,?????????????????????????????????????????????
sc <- spark_connect(master = "local", version = "2.3")
  ## ???????????????,spark_connect()???????????????Spark??????,
  ## ????????????????????????????????????sc; ??????,????????????sc??????Spark?????????

## Disconnecting
## spark_disconnect(sc)


## Using Spark
## ???mtcars??????????????????Apache Spark???
cars <- copy_to(sc, mtcars)
## access data
cars
  ## copy_to()
  ## ???????????????sc?????????????????????????????????spark_connect()???????????????Spark???????????????
  ## ?????????????????????????????????Spark???????????????





## Web Interface
## ?????????Spark???????????????R??????????????????; ?????????????????????????????????Spark???Web???????????????
## access Web Interface
spark_web(sc)





## Analysis
## using Spark from R to analyze data, can use SQL (Structured Query Language) 
## or dplyr (a grammar of data manipulation)
## use SQL through the DBI package
library(DBI)
dbGetQuery(sc, "SELECT count(*) FROM mtcars")

library(dplyr)
select(cars, hp, mpg) %>%
  sample_n(100) %>%
  collect() %>%
  plot()



## Modeling
## ml_linear_regression {sparklyr}
model <- ml_linear_regression(cars, mpg ~ hp)
model %>%
  ml_predict(copy_to(sc, data.frame(hp = 250 + 10 * 1:10))) %>%
  transmute(hp = hp, mpg = prediction) %>%
  full_join(select(cars, hp, mpg)) %>%
  collect() %>%
  plot()


## Data
## ??????????????????????????????Spark???
##  export our cars dataset
spark_write_csv(cars, "cars.csv")
## read back from the local file system
cars <- spark_read_csv(sc, "cars.csv")
 

## Streaming:  dynamic datasets
## ??????????????????????????????????????????,????????????????????????
## ?????????????????????Kafka(???????????????????????????)????????????????????????????????????????????????????????????
## ????????????,???????????????????????????Input /?????????,????????????????????????,????????????????????????????????????
## ???????????????,??????????????????input /????????????????????????,???R????????????????????????,?????????????????????output /?????????:
dir.create("input")
write.csv(mtcars, "input/cars_1.csv", row.names = F)
stream <- stream_read_csv(sc, "input/") %>%
  select(mpg, cyl, disp) %>%
  stream_write_csv("output/")

## ??????????????????????????????????????????????????????spark_apply()?????????????????????
## [1] "part-00000-40270e83-c741-476d-a531-244c668e55ba-c000.csv"
dir("output", pattern = ".csv")

## ????????????????????????????????????/??????,Spark?????????????????????????????????
## Write more data into the stream source
write.csv(mtcars, "input/cars_2.csv", row.names = F)

## ???????????????????????????
dir("output", pattern = ".csv")
## [1] "part-00000-40270e83-c741-476d-a531-244c668e55ba-c000.csv"
## [2] "part-00000-92b0ab2f-ece0-4e33-8e27-f6168d123254-c000.csv"

## Stop the stream
stream_stop(stream)










# Analysis
## open a new local connection.
library(sparklyr)
library(dplyr)
sc <- spark_connect(master = "local", version = "2.3")

## ??????,???????????????R??????????????????????????????????????????; 
## ??????Spark???,??????????????????Spark???,?????????R??????
## ????????????Spark?????????????????????????????????,??????????????????????????????Spark
## ????????????????????????Spark????????????????????????????????????????????????
## using real clusters, you should use copy_to() to transfer only small tables from 
## large data transfers should be performed with specialized data transfer tools.
cars <- copy_to(sc, mtcars)

## Data wrangling uses transformations 
summarize_all(cars, mean) %>%
  show_query()

cars %>%
  mutate(transmission = ifelse(am == 0, "automatic", "manual")) %>%
  group_by(transmission) %>%
  summarise_all(mean)  %>%
  show_query()


summarise(cars, mpg_percentile = percentile(mpg, 0.25)) %>%
  show_query()

summarise(cars, mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75)))

summarise(cars, mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75))) %>%
  mutate(mpg_percentile = explode(mpg_percentile))


library(ggplot2)
car_group <- cars %>%
  group_by(cyl) %>%
  summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
  collect() %>%
  print()
ggplot(aes(as.factor(cyl), mpg), data = car_group) + 
  geom_col(fill = "#999999") + coord_flip()



cars %>% 
  ml_linear_regression(mpg ~ hp + cyl) %>%
  summary() 
## generalized linear model:
cars %>% 
  ml_generalized_linear_regression(mpg ~ hp + cyl) %>%
  summary()


cached_cars <- cars %>% 
  mutate(cyl = paste0("cyl_", cyl)) %>%
  compute("cached_cars")
cached_cars %>%
  ml_linear_regression(mpg ~ .) %>%
  summary()



## download this dataset as follows:
download.file(
  "https://github.com/r-spark/okcupid/raw/master/profiles.csv.zip",
  "okcupid.zip")
unzip("okcupid.zip", exdir = "data")
unlink("okcupid.zip")



sc <- spark_connect(master = "local", version = "2.3")

okc <- spark_read_csv(
  sc, 
  "data/profiles.csv", 
  escape = "\"", 
  memory = FALSE,
  options = list(multiline = TRUE)
) %>%
  mutate(
    height = as.numeric(height),
    income = ifelse(income == "-1", NA, as.numeric(income))
  ) %>%
  mutate(sex = ifelse(is.na(sex), "missing", sex)) %>%
  mutate(drinks = ifelse(is.na(drinks), "missing", drinks)) %>%
  mutate(drugs = ifelse(is.na(drugs), "missing", drugs)) %>%
  mutate(job = ifelse(is.na(job), "missing", job))

## glimpse()??????????????????
glimpse(okc)

## add our response variable
okc <- okc %>%
  mutate(
    not_working = ifelse(job %in% c("student", "unemployed", "retired"), 1 , 0)
  )
okc %>% 
  group_by(not_working) %>% 
  tally()

##  split of our data into a training set and a testing set 
data_splits <- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 42)
okc_train <- data_splits$training
okc_test <- data_splits$testing

##  obtain numerical summaries of specific columns
sdf_describe(okc_train, cols = c("age", "income"))

ggplot(okc_train, aes(x = age))+geom_histogram()

library(dbplot)
dbplot_histogram(okc_train, age)





## ??????????????????????????????????????????:alcohol use and drug use.
## ??????sdf_crosstab()???????????????:
contingency_tbl <- okc_train %>% 
  sdf_crosstab("drinks", "drugs") %>%
  collect()



scale_values <- okc_train %>%
  summarize(
    mean_age = mean(age),
    sd_age = sd(age)
  ) %>%
  collect()

okc_train <- okc_train %>%
  mutate(scaled_age = (age - !!scale_values$mean_age) /
           !!scale_values$sd_age)
okc_train


## create a list of subsets from our okc_train table:
vfolds <- sdf_random_split(
  okc_train,
  weights = purrr::set_names(rep(0.1, 10), paste0("fold", 1:10)),
  seed = 42
)

## create our first analysis/assessment split as follows
analysis_set <- do.call(rbind, vfolds[2:10])
assessment_set <- vfolds[[1]]




sc <- spark_connect(master = "local", version = "2.3")

okc <- spark_read_csv(
  sc, 
  "data/profiles.csv", 
  escape = "\"", 
  memory = FALSE,
  options = list(multiline = TRUE)
) %>%
  mutate(
    height = as.numeric(height),
    income = ifelse(income == "-1", NA, as.numeric(income))
  ) %>%
  mutate(sex = ifelse(is.na(sex), "missing", sex)) %>%
  mutate(drinks = ifelse(is.na(drinks), "missing", drinks)) %>%
  mutate(drugs = ifelse(is.na(drugs), "missing", drugs)) %>%
  mutate(job = ifelse(is.na(job), "missing", job))

## glimpse()??????????????????
glimpse(okc)

## add our response variable
okc <- okc %>%
  mutate(
    not_working = ifelse(job %in% c("student", "unemployed", "retired"), 1 , 0)
  )
okc %>% 
  group_by(not_working) %>% 
  tally()

##  split of our data into a training set and a testing set 
data_splits <- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 42)
okc_train <- data_splits$training
okc_test <- data_splits$testing

 
essay_cols <- paste0("essay", 0:9)
essays <- okc %>%
  select(!!essay_cols)
essays %>% 
  glimpse()

stop_words <- ml_default_stop_words(sc) %>%
  c(
    "like", "love", "good", "music", "friends", "people", "life",
    "time", "things", "food", "really", "also", "movies"
  )

lda_model <-  ml_lda(essays, ~ words, k = 6, max_iter = 1, min_token_length = 4, 
                     stop_words = stop_words, min_df = 5)
betas <- tidy(lda_model)
betas
 

