# fit model with interaction
fit <- lm(neg_c_7 ~ c12hour + barthtot * c161sex, data = efc)
plot_model(fit, type = "pred", terms = c("barthtot", "c161sex"))
plot_model(fit, type = "pred", terms = c("barthtot", "c161sex"), colors = "black")
plot_model(fit, type = "pred", terms = c("barthtot", "c161sex"))
str(efc)
str(efc[,c("c12hour", "c172code")])
data(mtcars)
mpg_model <- lm(mpg ~ log(hp), data = mtcars)
#（反向）转换预测值
# x-values and predictions based on exponentiated hp-values
plot_model(mpg_model, type = "pred", terms = "hp [exp]")
library(splines)
data(women)
fm1 <- lm(weight ~ bs(height, df = 5), data = women)
plot_model(fm1, type = "pred", terms = "height")
women$height2 <- women$height^5
fm1 <- lm(weight ~ height2, data = women)
plot_model(fm1, type = "pred", terms = "height2")
fm1 <- lm(weight ~ bs(height, df = 5), data = women)
plot_model(fm1, type = "pred", terms = "height")
data(efc)
efc$c172code <- sjlabelled::as_factor(efc$c172code)
fit <- lm(neg_c_7 ~ c12hour + c172code, data = efc)
fit
summary(fit)
## switch the order of these terms on the terms-argument and use type = "pred".
plot_model(fit, type = "pred", terms = c("c161sex", "barthtot [0, 100]"))
data(efc)
theme_set(theme_sjplot())
# make categorical
efc$c161sex <- to_factor(efc$c161sex)
# fit model with interaction
fit <- lm(neg_c_7 ~ c12hour + barthtot * c161sex, data = efc)
## switch the order of these terms on the terms-argument and use type = "pred".
plot_model(fit, type = "pred", terms = c("c161sex", "barthtot [0, 100]"))
## switch the order of these terms on the terms-argument and use type = "pred".
## 切换顺序
plot_model(fit, type = "pred", terms = c("c161sex", "barthtot"))
# 补充其他的值
plot_model(fit, type = "int", mdrt.values = "meansd")
# fit model with interaction, switching terms in formula
# 默认情况下，对于连续变量，将最小值和最大值选择为分组级别
fit <- lm(neg_c_7 ~ c12hour + c161sex * barthtot, data = efc)
# 补充其他的值
plot_model(fit, type = "int", mdrt.values = "meansd")
library(parameters)
data(efc)
# find all variables from COPE-Index, which all have a "cop" in their
# variable name, and then plot that subset as likert-plot
mydf <- find_var(efc, pattern = "cop", out = "df")
plot_likert(mydf)
summarise(efc)
data(efc)
theme_set(theme_sjplot())
## Fitting a logistic regression model
# create binary response
y <- ifelse(efc$neg_c_7 < median(na.omit(efc$neg_c_7)), 0, 1)
# create data frame for fitting model
df <- data.frame(
y = to_factor(y),
sex = to_factor(efc$c161sex),
dep = to_factor(efc$e42dep),
barthel = efc$barthtot,
education = to_factor(efc$c172code)
)
# set variable label for response
set_label(df$y) <- "High Negative Impact"
# fit model
m1 <- glm(y ~., data = df, family = binomial(link = "logit"))
# Showing value labels
plot_model(m1, show.values = TRUE, value.offset = .4)
# Showing value labels
plot_model(m1, show.values = TRUE, value.offset = .1)
# Labelling the plot
# to use variable names even for labelled data
plot_model(m1, axis.labels = "", title = "my own title")
# Standardized estimates
# 线性模型，还可以使用type =“ std”或type =“ std2”来绘制标准化的beta系数。 这两个选项在系数标准化方式上有所不同。 type =“ std2”绘制标准化的beta值，但是标准化遵循Gelman（2008）的建议，将估算值除以两个标准偏差而不是一个标准偏差，从而重新缩放比例。
plot_model(m1, type = "std")
if (require("rstanarm", quietly = TRUE)) {
# make sure we apply a nice theme
library(ggplot2)
theme_set(theme_sjplot())
data(mtcars)
m <- stan_glm(mpg ~ wt + am + cyl + gear, data = mtcars, chains = 1)
# default model
plot_model(m)
# same model, with mean point estimate, dot-style for point estimate
# and different inner/outer probabilities of the HDI
plot_model(
m,
bpe = "mean",
bpe.style = "dot",
prob.inner = .4,
prob.outer = .8
)
}
# make sure we apply a nice theme
library(ggplot2)
theme_set(theme_sjplot())
data(mtcars)
m <- stan_glm(mpg ~ wt + am + cyl + gear, data = mtcars, chains = 1)
# default model
plot_model(m)
# same model, with mean point estimate, dot-style for point estimate
# and different inner/outer probabilities of the HDI
plot_model(
m,
bpe = "mean",
bpe.style = "dot",
prob.inner = .4,
prob.outer = .8
)
plot_model(
m1,
colors = "Accent",
show.values = TRUE,
value.offset = .4,
value.size = 4,
dot.size = 3,
line.size = 1.5,
vline.color = "blue",
width = 1.5
)
library(lme4)
data("sleepstudy")
data("efc")
efc$cluster <- as.factor(efc$e15relat)
efc$neg_c_7d <- ifelse(efc$neg_c_7 < median(efc$neg_c_7, na.rm = TRUE), 0, 1)
efc$cluster <- as.factor(efc$e15relat)
m3 <- glmer(
neg_c_7d ~ c160age + c161sex + e42dep + (1 | cluster),
data = efc,
family = binomial(link = "logit")
)
tab_model(m1, m3)
data("efc")
efc$neg_c_7d <- ifelse(efc$neg_c_7 < median(efc$neg_c_7, na.rm = TRUE), 0, 1)
efc$cluster <- as.factor(efc$e15relat)
m3 <- glmer(
neg_c_7d ~ c160age + c161sex + e42dep + (1 | cluster),
data = efc,
family = binomial(link = "logit")
)
tab_model(m1, m3)
data("sleepstudy")
data("efc")
efc$cluster <- as.factor(efc$e15relat)
## Unlike tables for non-mixed models, tab_models() adds additional information on the random effects to the table output for mixed models. You can hide these information with show.icc = FALSE and show.re.var = FALSE.
m1 <- lmer(neg_c_7 ~ c160age + c161sex + e42dep + (1 | cluster), data = efc)
data("efc")
efc$neg_c_7d <- ifelse(efc$neg_c_7 < median(efc$neg_c_7, na.rm = TRUE), 0, 1)
efc$cluster <- as.factor(efc$e15relat)
m3 <- glmer(
neg_c_7d ~ c160age + c161sex + e42dep + (1 | cluster),
data = efc,
family = binomial(link = "logit")
)
tab_model(m1, m3)
# load required packages
library(insight)
library(httr)
library(brms)
install.packages("brms")
library(brms)
library(brms)
m1 <- insight::download_model("brms_zi_2")
# data(epilepsy)
# set.seed(123)
# epilepsy$visit <- as.numeric(epilepsy$visit)
# epilepsy$Base2 <- sample(epilepsy$Base, nrow(epilepsy), replace = TRUE)
# f1 <- bf(Base ~ zAge + count + (1 |ID| patient))
# f2 <- bf(Base2 ~ zAge + Trt + (1 |ID| patient))
# m2 <- brm(f1 + f2 + set_rescor(FALSE), data = epilepsy)
m2 <- insight::download_model("brms_mv_3")
tab_model(m1)
tab_model(m1)
data(iris)
model <- lm(Petal.Length ~ Sepal.Length * Species + Sepal.Width, data = iris)
# compare standard errors to result from sandwich-package
unname(sqrt(diag(sandwich::vcovHC(model))))
## 也可以使用clubSandwich :: vcovCR（）实现方差-协方差矩阵的聚类稳健估计。
## 因此，当vcov.fun =“ CR”时，将调用clubSandwich包中的相关函数。 请注意，此功能需要指定簇参数。
# create fake-cluster-variable, to demonstrate cluster robust standard errors
iris$cluster <- factor(rep(LETTERS[1:8], length.out = nrow(iris)))
# cluster-robust estimation
tab_model(
model,
vcov.fun = "CR",
vcov.type = "CR1",
vcov.args = list(cluster = iris$cluster),
show.se = TRUE
)
# compare standard errors to result from clubSsandwich-package
unname(sqrt(diag(clubSandwich::vcovCR(model, type = "CR1", cluster = iris$cluster))))
unname(sqrt(diag(clubSandwich::vcovCR(model, type = "CR1", cluster = iris$cluster))))
# create fake-cluster-variable, to demonstrate cluster robust standard errors
iris$cluster <- factor(rep(LETTERS[1:8], length.out = nrow(iris)))
tab_model(
model,
vcov.fun = "CR",
vcov.type = "CR1",
vcov.args = list(cluster = iris$cluster),
show.se = TRUE
)
## 鲁棒估计可以与标准化相结合。 但是，鲁棒协方差矩阵估计仅适用于show.std =“ std”。
# model parameters, robust estimation on standardized model
tab_model(
model,
show.std = "std",
vcov.fun = "HC"
)
library(lme4)
data(iris)
set.seed(1234)
iris$grp <- as.factor(sample(1:3, nrow(iris), replace = TRUE))
# fit example model
model <- lme4::lmer(
Sepal.Length ~ Species * Sepal.Width + Petal.Length + (1 | grp),
data = iris
)
# normal model parameters, like from 'summary()'
tab_model(model)
# model parameters, cluster robust estimation for mixed models
tab_model(
model,
vcov.fun = "CR",
vcov.type = "CR1",
vcov.args = list(cluster = iris$grp)
)
# model parameters, cluster robust estimation for mixed models
tab_model(
model,
vcov.fun = "CR",
vcov.type = "CR1",
vcov.args = list(cluster = iris$grp)
)
# model parameters, cluster robust estimation on standardized mixed model
tab_model(
model,
show.std = "std",
vcov.fun = "CR",
vcov.type = "CR1",
vcov.args = list(cluster = iris$grp)
)
# Index score with one component
tab_itemscale(mydf)
library(parameters)
# Compute PCA on Cope-Index, and retrieve
# factor indices for each COPE index variable
pca <- parameters::principal_components(mydf)
factor.groups <- parameters::closest_component(pca)
tab_itemscale(mydf, factor.groups)
data(efc)
m <- lm(barthtot ~ c160age + c12hour + c161sex + c172code, data = efc)
tab <- tab_model(m)
cat(tab$page.style)
cat(tab$page.content)
tab_model(
m,
CSS = list(
css.depvarhead = 'color: red;',
css.centeralign = 'text-align: left;',
css.firsttablecol = 'font-weight: bold;',
css.summary = 'color: blue;'
)
)
tab_model(m, CSS = css_theme("cells"))
# Index score with one component
tab_itemscale(mydf)
## interprete the output
## item difficulty should range between 0.2 and 0.8. Ideal value is p+(1-p)/2 (which mostly is between 0.5 and 0.8)
## item discrimination, acceptable values are 0.2 or higher; the closer to 1 the better
## in case the total Cronbach’s Alpha value is below the acceptable cut-off of 0.7 (mostly if an index has few items), the mean inter-item-correlation is an alternative measure to indicate acceptability; satisfactory range lies between 0.2 and 0.4
# Index score with more than one component
library(parameters)
# Compute PCA on Cope-Index, and retrieve
# factor indices for each COPE index variable
pca <- parameters::principal_components(mydf)
factor.groups <- parameters::closest_component(pca)
tab_itemscale(mydf, factor.groups)
# Adding further statistics
tab_itemscale(mydf, factor.groups, show.shapiro = TRUE, show.kurtosis = TRUE)
### Diagnose
require(foreign)
require(MASS)
cdata <- read.dta("https://stats.idre.ucla.edu/stat/data/crime.dta")
summary(cdata)
summary(ols <- lm(crime ~ poverty + single, data = cdata))
cdata <- read.dta("https://stats.idre.ucla.edu/stat/data/crime.dta")
bank<- data.frame(
y=c(1018.4,1258.9,1359.4,1545.6,1761.6,1960.8),
x1=c(159,42,95,102,104,108),
x2=c(223.1,269.4,297.1,330.1,337.9,400.5),
x3=c(500,370,430,390,330,310),
x4=c(112.3,146.4,119.9,117.8,122.3,167.0),
w=c(5,6,8,3,6,8)
)
# 简单相关系数矩阵
cor(bank[,c(2,3,4,5)],method = "pearson")
## 注意：仅凭数学上有意义和很高的相关性以及漂亮的R方统计量，是不能认为结论正确的。
## 示例：Anscombe数据集
data(anscombe)
attach(anscombe)
anscombe
## 每对变量都具有相同的相关系数0.816
## cor(anscombe)
cor(x1, y1) #correlation of x1 and y1
cor(x2, y1) #correlation of x2 and y2
## 变量的统计图说明更多的问题
par(mfrow = c(2,2)) #create a 2x2 grid for plotting
plot(x1, y1, main = "Plot 1")
plot(x2, y2, main = "Plot 2")
plot(x3, y3, main = "Plot 3")
detach(anscombe)
dev.off()
bank.lm <- lm(y~x1+x2+x3+x4,data=bank)
fitted(bank.lm)    # vorhergesagte Werte
yhat <- fitted(bank.lm); segments(x1,yhat,x1,y,lty=2)    # lines(dist,yhat) # Werte identisch mit abline()
coef(bank.lm)      # 求模型系数
deviance(bank.lm)  # 计算残差平方和
residuals(bank.lm) # 计算残差
anova(bank.lm)     # 方差分析表
summary(bank.lm)   # 提取模型汇总资料
confint(bank.lm,
level=0.95)# Konfidenzintervall für β
yhat <- fitted(bank.lm)
yhat
segments(bank$x1,yhat,bank$x1,bank$y,lty=2)    # lines(dist,yhat) # Werte identisch mit abline()
# 残差图：Residuals vs Fitted   &   Normal Q-Q   &   标准化残差  &  Cook‘s distance
opar <- par()
par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(bank.lm)      # Diagnose - Grafiken
plot(bank.lm,which=1)
par(opar)
abline(bank.lm)
dev.off()
crime <- read.csv("~/Library/Mobile Documents/com~apple~CloudDocs/02. Programm/01 R_lernen/00 Data/crime.dta", sep="")
View(crime)
View(crime)
cdata <- read.dta("Library/Mobile\ Documents/com\~apple\~CloudDocs/02.\ Programm/01\ R_lernen/00\ Data
/crime.dta")
library(readr)
crime <- read_csv("~/Library/Mobile Documents/com~apple~CloudDocs/02. Programm/01 R_lernen/00 Data/crime.dta")
View(crime)
cdata <- read.dta("~/Library/Mobile Documents/com~apple~CloudDocs/02. Programm/01 R_lernen/00 Data/crime.dta")
summary(cdata)
summary(ols <- lm(crime ~ poverty + single, data = cdata))
opar <- par(mfrow = c(2,2), oma = c(0, 0, 1.1, 0))
plot(ols, las = 1)
par(opar)
ncvTest(bank.lm)           # 异方差检验
outlierTest(bank.lm)       # 异常值检验
vif(bank.lm)               # 共线性检验：VIF>10
durbinWatsonTest(bank.lm)  # 自相关性检验：Annahme cov（εi ，εj）=0
hatvalues(bank.lm)         # 高杠杆点: 杠杆值大于均值的2~3倍，对回归参数影响加大
cooks.distance(bank.lm)    # 强影响点
influence.measures(bank.lm)# 将离群点、高杠杆点、强影响点
influencePlot(bank.lm)
library(car)
crPlots(bank.lm)
# 偏残差图，判断因变量与自变量之间是否呈非线性关系，排除掉其他自变量的影响
## From these plots, we can identify observations 9, 25, and 51 as possibly problematic to our model.
## cooks.distance: cut-off point is 4/n
cdata[c(9, 25, 51), 1:2]
d1 <- cooks.distance(ols)
r <- stdres(ols)
a <- cbind(cdata, d1, r)
a[d1 > 4/51, ]
## 残差分析 print the ten observations with the highest absolute residual values.
rabs <- abs(r)
a <- cbind(cdata, d1, r, rabs)
asorted <- a[order(-rabs), ]
asorted[1:10, ]
data(water)
data(water)
library(corrplot)
bank.cor <- cor(bank)
bank.cor
corrplot(bank.cor, method = "ellipse")
## 另外一种常用的可视化方式是散点图矩阵，可以通过调用pairs()函数来实现。它可以使我 们看到比前面的相关性矩阵图更多的信息:
pairs(~ ., data = bank)
library(leaps)
fit <- lm(BSAAM ~ ., data = socal.water)
library(alr3)
install.packages("alr3")
library(alr3)
data(water)
socal.water <- water[ ,-1]
fit <- lm(BSAAM ~ ., data = socal.water)
fit <- lm(BSAAM ~ ., data = socal.water)
summary(fit)
## 使用最优子集法
sub.fit <- regsubsets(BSAAM ~ ., data = socal.water)
best.summary <- summary(sub.fit)
names(best.summary)
best.summary
## 对于模型选择有价值的函数还有which.min()和which.max()，它们分别给出具有某 个输出的最小值和最大值的模型
## 有6个特征的模型具有最小的RSS。本应如此，因为它有最多的输入， 输入越多，RSS越小。请注意，增加特征必然会减少RSS!而且必然会增加R方。我们可以添加 一个完全不相关的特征，比如洛杉矶湖人队的胜场数，RSS也会减少，R方也会增加。变动值可 能微不足道，但聊胜于无
which.min(best.summary$rss)
par(mfrow = c(1,2))
plot(best.summary$cp, xlab = "number of features", ylab = "cp")
plot(sub.fit, scale = "Cp")
best.fit <- lm(BSAAM ~ APSLAKE + OPRC + OPSLAKE, data = socal.water)
summary(best.fit)
par(mfrow = c(2,2))
plot(best.fit)
par(mfrow = c(2,2))
plot(best.fit)
## F统计量和所有t检验都具有显著的p值。通过了第一个检验，我们即 可生成诊断图
dev.off()
par(mfrow = c(2,2))
plot(best.fit)
dev.off()
vif(best.fit)
## OPRC和OPSLAKE存在潜在的共线性问题(VIF值大于5)
## 两个变量的关系图揭示了问题的根源
plot(socal.water$OPRC, socal.water$OPSLAKE, xlab = "OPRC", ylab = "OPSLAKE")
## 看一下最优子集法 中生成的修正R方的值就会发现，APSLAKE和OPSLAKE组成的两变量模型的值为0.90，加入 OPRC之后仅有一个微不足道的提升，到了0.92
best.summary$adjr2
## 看一下这个两变量模型及其假设检验结果
fit.2 <- lm(BSAAM ~ APSLAKE + OPSLAKE, data = socal.water)
fit.2 <- lm(BSAAM ~ APSLAKE + OPSLAKE, data = socal.water)
summary(fit.2)
vif(fit.2)
par(mfrow=c(2,2))
plot(fit.2)
dev.off()
## 模型是显著的，诊断图中也没发现什么问题，共线性问题应该得到了解决。再使用vif()函数检查一下
vif(fit.2)
library(lmtest)
bptest(fit.2)
plot(fit.2$fitted.values, socal.water$BSAAM, xlab = "predicted", ylab = "actual",
main = "Predicted vs. Actual")
socal.water["Actual"] = water$BSAAM
socal.water$Forecast = predict(fit.2)
head(socal.water)
library(ggplot2)
ggplot(socal.water, aes(x = Forecast, y = Actual)) +
geom_point() + geom_smooth(method = lm) +
labs(title = "Forecast versus Actuals")
library(MPV)
install.packages("MPV")
PRESS.best = sum((resid(best.fit)/(1 - hatvalues(best.fit)))^2)
PRESS.fit.2 = sum((resid(fit.2)/(1 - hatvalues(fit.2)))^2)
PRESS.best
PRESS.fit.2
library(ISLR)
data(Carseats)
sales.fit = lm(Sales ~ Advertising + ShelveLoc, data = Carseats)
summary(sales.fit)
contrasts(Carseats$ShelveLoc)
install.packages("ISLR")
library(ISLR)
data(Carseats)
sales.fit = lm(Sales ~ Advertising + ShelveLoc, data = Carseats)
summary(sales.fit)
## 如何对指标特征进行编码，可以使用contrasts()函数
contrasts(Carseats$ShelveLoc)
library(MASS)
data(Boston)
value.fit <- lm(medv ~ lstat * age, data = Boston)
summary(value.fit)
ncvTest(bank.lm)           # 异方差检验
outlierTest(bank.lm)       # 异常值检验
vif(bank.lm)               # 共线性检验：VIF>10
durbinWatsonTest(bank.lm)  # 自相关性检验：Annahme cov（εi ，εj）=0
hatvalues(bank.lm)         # 高杠杆点: 杠杆值大于均值的2~3倍，对回归参数影响加大
cooks.distance(bank.lm)    # 强影响点
influence.measures(bank.lm)# 将离群点、高杠杆点、强影响点
influencePlot(bank.lm)
?ncvTest
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(mvtnorm)
set.seed(977) # this makes the simulation exactly reproducible
ni     = 100  # 100 people
nj     =  10  # 10 week study
id     = rep(1:ni, each=nj)
id
cond   = rep(c("control", "diet"), each=nj*(ni/2))
cond
base   = round(rep(rnorm(ni, mean=250, sd=10), each=nj))
base
week   = rep(1:nj, times=ni)
y      = round(base + rnorm(ni*nj, mean=0, sd=1))
# MCAR
prop.m = .07  # 7% missingness
mcar   = runif(ni*nj, min=0, max=1)
mcar
prop.m = .07  # 7% missingness
mcar   = runif(ni*nj, min=0, max=1)
y.mcar = ifelse(mcar<prop.m, NA, y)  # unrelated to anything,if smaller than 0.07 missing
View(cbind(id, week, cond, base, y, y.mcar))
# MAR
y.mar = matrix(y, ncol=nj, nrow=ni, byrow=TRUE)
y.mar
dif1 = y.mar[i,j-2]-y.mar[i,j-3]
for(i in 1:ni){
for(j in 4:nj){
dif1 = y.mar[i,j-2]-y.mar[i,j-3]
dif2 = y.mar[i,j-1]-y.mar[i,j-2]
if(dif1>0 & dif2>0){  # if weight goes up twice, drops out
y.mar[i,j:nj] = NA;  break
}
}
}
y.mar
y.mar = as.vector(t(y.mar))
y.mar
View(cbind(id, week, cond, base, y, y.mar))
